# ğŸ§  Awesome Visual Reasoning Datasets

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)  
[![Hugging Face](https://img.shields.io/badge/ğŸ¤—-HuggingFace-yellow)](https://huggingface.co/) 
[![arXiv](https://img.shields.io/badge/arXiv-Resources-red)](https://arxiv.org/)  
[![Stars](https://img.shields.io/github/stars/awesome-ai/awesome-visual-reasoning?style=social)](https://github.com/awesome-ai/awesome-visual-reasoning/stargazers)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/awesome-ai/awesome-visual-reasoning/pulls)
[![License](https://img.shields.io/github/license/awesome-ai/awesome-visual-reasoning)](LICENSE)

> ğŸ’¡ *Visual reasoning* refers to the ability of AI systems to **infer, interpret, and reason** about visual information beyond perception.  
> This repository curates high-quality datasets across multiple reasoning domains â€” math, science, spatial, commonsense, and multimodal logic â€” to support **VLM** and **reasoning model research**.

---

## ğŸ“– Table of Contents
- [ğŸ§® Mathematical Reasoning](#-mathematical-reasoning)
- [ğŸ”¬ Scientific Reasoning](#-scientific-reasoning)
- [ğŸ“ Spatial & Geometric Reasoning](#-spatial--geometric-reasoning)
- [ğŸ§© Commonsense Reasoning](#-commonsense-reasoning)
- [ğŸ“Š Chart, Diagram & Figure Reasoning](#-chart-diagram--figure-reasoning)
- [ğŸŒ† Scene & Object Understanding](#-scene--object-understanding)
- [ğŸ’¬ Multimodal Logical Reasoning](#-multimodal-logical-reasoning)
- [ğŸ§± Synthetic & Programmatic Datasets](#-synthetic--programmatic-datasets)
- [ğŸ”— Hybrid Datasets](#-hybrid)
- [ğŸ“š References](#-references)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸª¶ Citation](#-citation)

---

<details open>
<summary><h2>ğŸ§® Mathematical Reasoning</h2></summary>

| Dataset | Description | Source | Modality | Size | Task Type |
|----------|--------------|---------|-----------|------|-----------|
| **MathVista** | Integrates visual & textual math reasoning tasks (geometry, algebra, puzzles). | [ğŸŒ Website](https://mathvista.github.io/) â€¢ [ğŸ¤— HF](https://huggingface.co/datasets/MathVista) | Image + Text | 6.5k | Multimodal QA |
| **GeoQA** | Geometry problem-solving with diagram understanding. | [GitHub](https://github.com/TAL-AI/GeoQA) â€¢ [ğŸ¤— HF](https://huggingface.co/datasets/TAL-AI/GeoQA) | Diagram + Text | 5.8k | Diagram QA |
| **MathVerse** | Complex visual-textual math reasoning with compositional logic. | [ğŸŒ Website](https://mathverse.github.io/) | Image + Text | 12k | QA + CoT reasoning |
| **We-Math** | Large-scale step-by-step multimodal math reasoning dataset. | [ğŸŒ Website](https://we-math.github.io/) â€¢ [ğŸ¤— HF](https://huggingface.co/datasets/we-math/WeMath) | Image + Text | 50k | Reasoning QA |
| **We-Math 2.0** | Unified MathBook system with RL-based multimodal reasoning tasks. | [ğŸ¤— HF](https://huggingface.co/datasets/we-math/WeMath2.0) | Image + Text | â€“ | CoT, RL fine-tuning |
| **MM-MathInstruct** | Instruction-tuning dataset for multimodal math reasoning models. | [ğŸ¤— HF](https://huggingface.co/MathLLMs/MM-MathInstruct) | Image + Text | â€“ | Instruction tuning |
| **Geo170K** | Large synthetic geometric dataset for diagram-based QA and reasoning. | [ğŸ¤— HF](https://huggingface.co/datasets/G-LLaVA/Geo170K) | Image + Text | ~170K | Geometry QA |

</details>

---

<details>
<summary><h2>ğŸ”¬ Scientific Reasoning</h2></summary>

| Dataset | Description | Source | Modality | Size | Task Type |
|----------|-------------|---------|-----------|------|-----------|
| **ScienceQA** | Multimodal QA over science topics (images, diagrams, text). | [ğŸŒ Website](https://scienceqa.github.io/) â€¢ [ğŸ¤— HF](https://huggingface.co/datasets/derek-thomas/ScienceQA) | Image + Text | 21k | QA + Explanation |
| **AI2D** | Diagram-based QA for scientific diagram understanding. | [ğŸŒ Website](https://allenai.org/data/ai2d) â€¢ [ğŸ¤— HF](https://huggingface.co/datasets/allenai/ai2d) | Diagram + Text | 5k | Diagram QA |
| **ScienceDiagramsQA** | Diagram-centric reasoning dataset in science. | [GitHub](https://github.com/SciQA/ScienceDiagramsQA) | Diagram + Text | 7k | QA |
| **TQA (TextbookQA)** | QA derived from textbooks combining text and diagrams. | [ğŸŒ Website](https://tqa.cs.washington.edu/) | Diagram + Text | 26k | Text + Diagram QA |

</details>

---

<details>
<summary><h2>ğŸ“ Spatial & Geometric Reasoning</h2></summary>

| Dataset | Description | Source | Modality | Size | Task Type |
|----------|-------------|---------|-----------|------|-----------|
| **CLEVR** | Synthetic dataset testing compositional visual reasoning. | [ğŸŒ Website](https://cs.stanford.edu/people/jcjohns/clevr/) â€¢ [ğŸ¤— HF](https://huggingface.co/datasets/clevr) | Image | 100k | Visual QA |
| **GQA** | Real-world compositional reasoning dataset. | [ğŸŒ Website](https://cs.stanford.edu/people/dorarad/gqa/index.html) â€¢ [ğŸ¤— HF](https://huggingface.co/datasets/gqa) | Image | 22M | Scene QA |
| **ShapeWorld** | Synthetic shapes dataset for abstract reasoning. | [GitHub](https://github.com/AlexKuhnle/ShapeWorld) | Image | Variable | Description QA |
| **CoGenT** | CLEVR variant testing compositional generalization. | [ğŸŒ Website](https://cs.stanford.edu/people/jcjohns/clevr/) | Image | 100k | Generalization QA |

</details>

---

<details>
<summary><h2>ğŸ§© Commonsense Reasoning</h2></summary>

| Dataset | Description | Source | Modality | Size | Task Type |
|----------|-------------|---------|-----------|------|-----------|
| **VCR** | Visual commonsense reasoning over images and text. | [ğŸŒ Website](https://visualcommonsense.com/) â€¢ [ğŸ¤— HF](https://huggingface.co/datasets/visualcommonsense/VCR) | Image + Text | 290k | QA + Rationale |
| **OK-VQA** | QA requiring commonsense knowledge beyond the image. | [ğŸŒ Website](https://okvqa.allenai.org/) â€¢ [ğŸ¤— HF](https://huggingface.co/datasets/okvqa) | Image + Text | 14k | Open QA |
| **A-OKVQA** | Extended OK-VQA with multiple-choice and rationale explanations. | [ğŸŒ Website](https://allenai.org/project/a-okvqa) â€¢ [ğŸ¤— HF](https://huggingface.co/datasets/a-okvqa) | Image + Text | 25k | QA + Rationale |

</details>

---

<details>
<summary><h2>ğŸ“Š Chart, Diagram & Figure Reasoning</h2></summary>

| Dataset | Description | Source | Modality | Size | Task Type |
|----------|-------------|---------|-----------|------|-----------|
| **ChartQA** | Reasoning over charts with natural language questions. | [GitHub](https://github.com/vis-nlp/ChartQA) â€¢ [ğŸ¤— HF](https://huggingface.co/datasets/vis-nlp/ChartQA) | Chart + Text | 20k | Chart QA |
| **FigureQA** | Synthetic figure reasoning via attribute comparison. | [ğŸŒ Website](https://datasets.maluuba.com/FigureQA) | Chart | 100k | Figure reasoning |
| **PlotQA** | Real-world plot-based QA emphasizing data reasoning. | [GitHub](https://github.com/NiteshMethani/PlotQA) | Chart + Text | 28k | Data reasoning QA |
| **DVQA** | Bar chart reasoning with compositional questions. | [ğŸŒ Website](https://www.cs.cmu.edu/~dvqa/) | Chart + Text | 3M | Chart QA |

</details>

---

<details>
<summary><h2>ğŸŒ† Scene & Object Understanding</h2></summary>

| Dataset | Description | Source | Modality | Size | Task Type |
|----------|-------------|---------|-----------|------|-----------|
| **Visual7W** | Visual QA emphasizing grounding and reasoning. | [ğŸŒ Website](https://ai.stanford.edu/~yukez/visual7w/) | Image + Text | 327k | QA |
| **VQA v2** | Large-scale benchmark with diverse reasoning types. | [ğŸŒ Website](https://visualqa.org/) â€¢ [ğŸ¤— HF](https://huggingface.co/datasets/vqa) | Image + Text | 1.1M | QA |
| **VizWiz** | Real-world dataset from blind users' photos. | [ğŸŒ Website](https://vizwiz.org/) â€¢ [ğŸ¤— HF](https://huggingface.co/datasets/vizwiz) | Image + Text | 31k | QA |

</details>

---

<details>
<summary><h2>ğŸ’¬ Multimodal Logical Reasoning</h2></summary>

| Dataset | Description | Source | Modality | Size | Task Type |
|----------|-------------|---------|-----------|------|-----------|
| **VisDial** | Dialogue-based visual reasoning. | [ğŸŒ Website](https://visualdialog.org/) â€¢ [ğŸ¤— HF](https://huggingface.co/datasets/visdial) | Image + Dialogue | 120k | Dialog reasoning |
| **IconQA** | IQ-test style reasoning with diagrams and analogies. | [ğŸŒ Website](https://iconqa.github.io/) | Diagram + Text | 40k | Analogy QA |
| **RAVEN** | Abstract reasoning inspired by Ravenâ€™s Progressive Matrices. | [GitHub](https://github.com/WellyZhang/RAVEN) | Diagram | 70k | Abstract reasoning |
| **I-RAVEN** | Improved RAVEN dataset with diverse pattern distributions. | [GitHub](https://github.com/zhangjiqingyun/I-RAVEN) | Diagram | 100k | Logical reasoning |

</details>

---

<details>
<summary><h2>ğŸ§± Synthetic & Programmatic Datasets</h2></summary>

| Dataset | Description | Source | Modality | Size | Task Type |
|----------|-------------|---------|-----------|------|-----------|
| **CLEVRER** | Video-based causal reasoning extension of CLEVR. | [ğŸŒ Website](https://clevrer.csail.mit.edu/) | Video + Text | 20k | Causal reasoning |
| **IntPhys** | Tests intuitive physics understanding. | [ğŸŒ Website](https://intphys.com/) | Video | 10k | Physical reasoning |
| **PhysicalIQ** | Visual reasoning about physical interactions and stability. | [ğŸŒ Website](https://physicaliq.github.io/) | Image + Video | 12k | Physics QA |

</details>

---

<details>
<summary><h2>ğŸ”— Hybrid Datasets</h2></summary>

| Dataset | Description | Source | Modality | Size | Task Type |
|----------|-------------|---------|-----------|------|-----------|
| **MMR1** | Multimodal long-CoT reasoning dataset for SFT + RL pipelines. | [ğŸ¤— HF](https://huggingface.co/datasets/MMR1) | Image + Text | Millions | Long CoT QA |
| **Vision-G1** | Multi-domain RL-ready reasoning dataset curated from 46 sources (math, spatial, GUI, commonsense, etc.). | [ğŸ¤— HF](https://huggingface.co/datasets/vision-g1) â€¢ [arXiv](https://arxiv.org/abs/2410.11890) | Image + Text | â€“ | Multimodal QA, RL training |

</details>

---

## ğŸ“š References

- *Visual Reasoning Datasets and Benchmarks* â€” AI2, Stanford, Meta AI, DeepMind, Tsinghua University.  
- **Surveys**
  - [A Survey on Visual Reasoning (2023)](https://arxiv.org/abs/2306.04862)  
  - [Multimodal Reasoning in VLMs (2024)](https://arxiv.org/abs/2402.08000)

---

## ğŸ¤ Contributing

Contributions are warmly welcome! ğŸ’›  
Please open an **issue** or **pull request** to:
- Add new visual reasoning datasets  
- Update dataset metadata or Hugging Face links  
- Suggest improved classification or structure  

---

## ğŸª¶ Citation

```bibtex
@misc{awesome_visual_reasoning,
  title={Awesome Visual Reasoning Datasets},
  author={Yun Zhu},
  year={2025},
  url={https://github.com/awesome-ai/awesome-visual-reasoning}
}
